Problem (mixed_precision_accumulation): 1 point
Problem:
    Run the following code and commment on the (accuracy of the) results.
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
    print(s)
    s = torch.tensor(0,dtype=torch.float16)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
    print(s)
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
    print(s)
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
    print(s)
    Deliverable: A 2-3 sentence response

ans:
    tensor(10.0001)
    tensor(9.9531, dtype=torch.float16)
    tensor(10.0021)
    tensor(10.0021)


Problem (benchmarking_mixed_precision):
Problem:
    (a) Consider the following model:
    1 class ToyModel(nn.Module):
    2 def __init__(self, in_features: int, out_features: int):
    3 super().__init__()
    4 self.fc1 = nn.Linear(in_features, 10, bias=False)
    5 self.ln = nn.LayerNorm(10)
    6 self.fc2 = nn.Linear(10, out_features, bias=False)
    7 self.relu = nn.ReLU()
    8
    9 def forward(self, x):
    10 x = self.relu(self.fc1(x))
    11 x = self.ln(x)
    12 x = self.fc2(x)
    13 return x
    Suppose we are training the model on a GPU and that the model parameters are originally in
    FP32. We’d like to use autocasting mixed precision with FP16. What are the data types of:
    • the model parameters within the autocast context,
    • the output of the first feed-forward layer (ToyModel.fc1),
    • the output of layer norm (ToyModel.ln),
    • the model’s predicted logits,
    • the loss,
    • and the model’s gradients?
    Deliverable: The data types for each of the components listed above.
    (b) You should have seen that FP16 mixed precision autocasting treats the layer normalization layer
    differently than the feed-forward layers. What parts of layer normalization are sensitive to mixed
    precision? If we use BF16 instead of FP16, do we still need to treat layer normalization differently?
    Why or why not?
    Deliverable: A 2-3 sentence response.
    (c) Modify your benchmarking script to optionally run the model using mixed precision with BF16.
    Time the forward and backward passes with and without mixed-precision for each language model
    size described in §1.1.2. Compare the results of using full vs. mixed precision, and comment on
    any trends as model size changes. You may find the nullcontext no-op context manager to be
    useful.
    Deliverable: A 2-3 sentence response with your timings and commentary

ans:
    (a)
    | 组件                                  | 数据类型（dtype）     | 原因解释                                                 |
    | ----------------------------------- | --------------- | ---------------------------------------------------- |
    | **模型参数**                            | `torch.float32` | AMP 不会改变参数存储精度。参数仍为 FP32，以避免累积误差。                    |
    | **fc1 输出 (`ToyModel.fc1`)**         | `torch.float16` | Linear 是 matmul 类算子，AMP 会将输入和计算自动转为 FP16。            |
    | **LayerNorm 输出 (`ToyModel.ln`)**    | `torch.float32` | LayerNorm 在 AMP 下强制保持 FP32 精度，因为归一化与除法操作对精度敏感。       |
    | **模型预测 logits (`ToyModel.fc2` 输出)** | `torch.float16` | fc2 是线性层，在 AMP 下以 FP16 执行（输入 FP32 会被临时 cast 为 FP16）。 |
    | **Loss**                            | `torch.float32` | 损失函数通常会在 FP32 下计算（如 `CrossEntropyLoss` 会输出 FP32）。    |
    | **Gradients**                       | `torch.float32` | 梯度在 FP32 master 参数上累积与更新，保持稳定性。                      |

    (b)
    使用BF16时的处理：使用BF16时通常不需要特殊对待LayerNorm。因为BF16具有与FP32相同的指数范围(∼10⁻³⁸ to 10³⁸)，
    虽然精度较低但数值稳定性好，能够安全处理平方和除法操作而不会溢出。

    (c)
    在修改基准测试脚本以支持BF16混合精度时，主要步骤和预期结果如下：
    实现方法：使用torch.autocast上下文管理器，对前向传播和损失计算启用BF16，同时保持优化器在FP32中更新参数。
    预期计时趋势：
    小模型：混合精度可能带来轻微加速或无明显优势
    中大模型：混合精度应显示明显加速（1.3-1.8倍），因为BF16减少了内存带宽需求和计算开销
    超大模型：加速效果最显著，因为内存节省避免了GPU内存交换

    batch_size == 4
    ==== Benchmark Summary (ms per step) ====
    Forward:   708.74 ± 70.38
    Backward:  1122.31 ± 231.31
    Optimizer: 150.70 ± 19.64
    Total:     1986.32 ± 189.67
    =========================================

    batch_size == 2
    ==== Benchmark Summary (ms per step) ====
    Forward:   61.60 ± 0.32
    Backward:  129.18 ± 0.52
    Optimizer: 104.80 ± 0.76
    Total:     304.43 ± 23.73
    =========================================

Problem:
    Problem (pytorch_attention): 2 points
    (a) Benchmark your attention implementation at different scales. Write a script that will:
    (a) Fix the batch size to 8 and don’t use multihead attention (i.e. remove the head dimension).
    (b) Iterate through the cartesian product of [16, 32, 64, 128] for the head embedding dimension dmodel, and [256, 1024, 4096, 8192, 16384] for the sequence length.
    (c) Create random inputs Q, K, V for the appropriate size.
    (d) Time 100 forward passes through attention using the inputs.
    (e) Measure how much memory is in use before the backward pass starts, and time 100 backward
    passes.
    (f) Make sure to warm up, and to call torch.cuda.synchronize() after each forward/backward
    pass.
    Report the timings (or out-of-memory errors) you get for these configurations. At what size do
    you get out-of-memory errors? Do the accounting for the memory usage of attention in one of the
    smallest configurations you find that runs out of memory (you can use the equations for memory
    usage of Transformers from Assignment 1). How does the memory saved for backward change
    with the sequence length? What would you do to eliminate this memory cost?
    Deliverable: A table with your timings, your working out for the memory usage, and a 1-2
    paragraph response.

Solution:
       d_model  seq_len  forward_time_ms  backward_time_ms  memory_before_backward_MB   status
    0       16      256         0.286360          0.636823                  19.125488  Success
    1       16     1024         1.712267          3.668833                  51.750488  Success
    2       32      256         0.269003          0.634968                  20.000488  Success
    3       32     1024         1.730001          3.741384                  55.250488  Success
    4       64      256         0.296972          0.674500                  21.750488  Success
    5       64     1024         1.775091          4.014287                  62.250488  Success

Problem:
    Problem (torch_compile): 2 points
    (a) Extend your attention benchmarking script to include a compiled version of your PyTorch implementation of attention, and compare its performance to the uncompiled version with the same
    configuration as the pytorch_attention problem above.
    Deliverable: A table comparing your forward and backward pass timings for your compiled
    attention module with the uncompiled version from the pytorch_attention problem above.
    (b) Now, compile your entire Transformer model in your end-to-end benchmarking script. How does
    the performance of the forward pass change? What about the combined forward and backward
    passes and optimizer steps?
    Deliverable: A table comparing your vanilla and compiled Transformer model.




Solution:
    (a)
           version  d_model  seq_len  forward_time_ms  backward_time_ms   status
    0   uncompiled       16      256         0.386119          1.463807  Success
    1     compiled       16      256         0.407028          0.812531  Success
    2   uncompiled       16     1024         2.222729          4.298615  Success
    3     compiled       16     1024         1.766145          4.475713  Success
    4   uncompiled       32      256         1.328003          1.709282  Success
    5     compiled       32      256         0.406444          0.776589  Success
    6   uncompiled       32     1024         2.013481          4.145408  Success
    7     compiled       32     1024         1.710641          3.203261  Success
    8   uncompiled       64      256         0.343549          0.793374  Success
    9     compiled       64      256         0.382316          0.719428  Success
    10  uncompiled       64     1024         1.908934          4.085565  Success
    11    compiled       64     1024         1.623893          3.269637  Success

    === Comparison Table (compiled vs uncompiled) ===
                    backward_time_ms            forward_time_ms           
    version                 compiled uncompiled        compiled uncompiled
    d_model seq_len                                                       
    16      256             0.812531   1.463807        0.407028   0.386119
            1024            4.475713   4.298615        1.766145   2.222729
    32      256             0.776589   1.709282        0.406444   1.328003
            1024            3.203261   4.145408        1.710641   2.013481
    64      256             0.719428   0.793374        0.382316   0.343549
            1024            3.269637   4.085565        1.623893   1.908934

    (b)
    === Running vanilla model ===
    Running 5 warm-up steps...
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.48s/it]
    Running 10 measurement steps...
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:39<00:00,  9.95s/it]

    ==== Benchmark Summary (ms per step) ====
    Forward:   3014.62 ± 160.60
    Backward:  6569.98 ± 398.94
    Optimizer: 360.32 ± 103.30
    Total:     9952.94 ± 436.59
    =========================================

    === Running compiled model ===
    Running 5 warm-up steps...
    0%|                                                                                                                              | 0/5 [00:00<?, ?it/s]/home/booft/pytorch-selflearning/assignment2-systems/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
    warnings.warn(
    W1017 16:19:05.785000 19210 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:06<00:00, 13.24s/it]
    Running 10 measurement steps...
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:10<00:00,  7.00s/it]

    ==== Benchmark Summary (ms per step) ====
    Forward:   944.12 ± 165.17
    Backward:  5701.60 ± 561.36
    Optimizer: 356.77 ± 63.94
    Total:     7003.06 ± 621.03
    =========================================

    ==== Comparison Table (ms per step) ====
            Forward (ms)  Backward (ms)  Optimizer (ms)   Total (ms)
    vanilla    3014.621091    6569.982600      360.318279  9952.937675
    compiled    944.122863    5701.600552      356.773639  7003.055859
    =========================================
    
Question:
    Problem (distributed_communication_single_node): 5 points
    Write a script to benchmark the runtime of the all-reduce operation in the single-node multi-process
    setup. The example code above may provide a reasonable starting point. Experiment with varying the
    following settings:
    Backend + device type: Gloo + CPU, NCCL + GPU.
    all-reduce data size: float32 data tensors ranging over 1MB, 10MB, 100MB, 1GB.
    Number of processes: 2, 4, or 6 processes.
    Resource requirements: Up to 6 GPUs. Each benchmarking run should take less than 5 minutes.

    Deliverable: Plot(s) and/or table(s) comparing the various settings, with 2-3 sentences of commentary about your results and thoughts about how the various factors interact.


Solution : 
    [Backend=gloo, Device=cpu, Size=1MB, World=2]
    Avg all-reduce time (per rank): [0.0025213719345629215, 0.002513241721317172]
    Mean time: 0.0025 s

    [Backend=gloo, Device=cpu, Size=10MB, World=2]
    Avg all-reduce time (per rank): [0.006701040081679821, 0.0067071677185595036]
    Mean time: 0.0067 s

    [Backend=gloo, Device=cpu, Size=100MB, World=2]
    Avg all-reduce time (per rank): [0.06403537094593048, 0.06402194499969482]
    Mean time: 0.0640 s

    [Backend=gloo, Device=cpu, Size=1MB, World=4]
    Avg all-reduce time (per rank): [0.004708766937255859, 0.0047458172775805, 0.004713201429694891, 0.004721975419670343]
    Mean time: 0.0047 s

    [Backend=gloo, Device=cpu, Size=10MB, World=4]
    Avg all-reduce time (per rank): [0.012697577476501465, 0.012697339057922363, 0.012678814120590687, 0.012694573029875755]
    Mean time: 0.0127 s

    [Backend=gloo, Device=cpu, Size=100MB, World=4]
    Avg all-reduce time (per rank): [0.13856235146522522, 0.1385645866394043, 0.13855834305286407, 0.13856299221515656]
    Mean time: 0.1386 s

    
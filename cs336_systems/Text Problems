Problem (nsys_profile): 5 points
Profile your forward pass, backward pass, and optimizer step using nsys with each of the model
sizes described in Table 1 and context lengths of 128, 256, 512 and 1024 (you may run out of memory
with some of these context lengths for the larger models, in which case just note it in your report).
(a) What is the total time spent on your forward pass? Does it match what we had measured before
with the Python standard library?
Deliverable: A 1-2 sentence response.
(b) What CUDA kernel takes the most cumulative GPU time during the forward pass? How many
times is this kernel invoked during a single forward pass of your model? Is it the same kernel
that takes the most runtime when you do both forward and backward passes? (Hint: look at the
“CUDA GPU Kernel Summary” under “Stats Systems View”, and filter using NVTX ranges to
identify which parts of the model are responsible for which kernels.)
Deliverable: A 1-2 sentence response.
(c) Although the vast majority of FLOPs take place in matrix multiplications, you will notice that
several other kernels still take a non-trivial amount of the overall runtime. What other kernels
besides matrix multiplies do you see accounting for non-trivial CUDA runtime in the forward
pass?
Deliverable: A 1-2 sentence response.
(d) Profile running one complete training step with your implementation of AdamW (i.e., the forward
pass, computing the loss and running a backward pass, and finally an optimizer step, as you’d do
during training). How does the fraction of time spent on matrix multiplication change, compared
to doing inference (forward pass only)? How about other kernels?
Deliverable: A 1-2 sentence response.
(e) Compare the runtime of the softmax operation versus the matrix multiplication operations within
the self-attention layer of your model during a forward pass. How does the difference in runtimes
compare to the difference in FLOPs?
Deliverable: A 1-2 sentence response.

forward_only:
 ** NVTX Range Summary (nvtx_sum):

 Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)    Min (ns)    Max (ns)   StdDev (ns)    Style            Range         
 --------  ---------------  ---------  ------------  ------------  ---------  ----------  ------------  -------  -----------------------
     50.0      43649639376         10  4364963937.6  4223782891.0  421926439  8847615231  4080879507.0  PushPop  :complete_training_step
     50.0      43648762123         10  4364876212.3  4223732576.5  421874868  8847494695  4080875246.4  PushPop  :forward_pass  

** CUDA GPU Kernel Summary (cuda_gpu_kern_sum):

 Time (%)  Total Time (ns)  Instances   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ----------------------------------------------------------------------------------------------------
     38.4      17045832703       1275  13369280.6  2154038.0    504237  235827861   24107302.0  ampere_sgemm_128x64_tn                                                                              
     18.4       8154731692       1080   7550677.5    68290.0     54274   39110498   13464119.5  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
     12.6       5597765630       2190   2556057.4    68690.0     53218   14820586    4155268.6  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      4.1       1820855674        360   5057932.4   415674.5    411242   16945025    6355868.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…
      4.0       1786405130        180   9924472.9   466476.0    403660   45383576   13458062.9  ampere_sgemm_128x128_tn                                                                             
      3.9       1726590763        180   9592170.9   740146.0    607826   35562227   12417865.0  ampere_sgemm_128x128_nn                                                                             
      2.8       1221336708        180   6785203.9   557758.0    554094   23527090    8354679.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::exp_kernel_cuda(at::TensorIterat…
      2.7       1187562694        180   6597570.5   557790.0    554862   26391070    8308030.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, floa…
      2.4       1062973073        180   5905406.0   551486.0    545199   25350114    7387919.3  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      2.3       1040675805        180   5781532.3   558575.5    549424   20966958    7158927.7  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      2.2        981510377        180   5452835.4   443787.0    436205   24585711    6993092.2  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      1.3        596492038        180   3313844.7   284839.5    278727   12960665    4306472.6  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::MaxOps<flo…
      1.3        586852707        180   3260292.8   278551.5    275911   11502482    4087952.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::sigmoid_kernel_cuda(at::TensorIt…
      1.3        586518738        180   3258437.4   278951.0    272775   10809760    3992329.0  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…
      1.1        499444755       1080    462448.8    52929.0     47553    5460882    1092859.6  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<float>, std::arr…
      0.7        301852011        375    804938.7    70786.0     66849    2834436    1022708.8  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::pow_tensor_scala…
      0.4        156705944        375    417882.5    43970.0     40353    7856051     634181.0  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::MeanOps<fl…


forward and backward and optimizer:
 Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)    StdDev (ns)   Style            Range         
 --------  ---------------  ---------  ------------  ------------  ----------  -----------  -----------  -------  -----------------------
     50.0      95975841174         10  9597584117.4  9557582265.5  9030617611  10352423477  443368177.0  PushPop  :complete_training_step
     32.7      62744013119         10  6274401311.9  6319356657.5  5706599581   6756404137  323919406.6  PushPop  :backward_pass         
     15.5      29663452644         10  2966345264.4  2969137784.5  2701582447   3377536829  229882692.9  PushPop  :forward_pass          
      1.8       3479453573         10   347945357.3   311895627.5   270134600    557741594   96006844.6  PushPop  :optimizer_step        
      0.0         86705263         10     8670526.3     1207115.0     1064862     74583523   23162151.4  PushPop  :loss_computation     

 ** CUDA GPU Kernel Summary (cuda_gpu_kern_sum):

 Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------
     14.4      20617342622       1260   16362970.3   10851749.0     487628   80181071   18736759.5  ampere_sgemm_128x64_nn                                                                              
     12.6      18001539881       1275   14118854.8    2113690.0     504397  220880939   27750748.8  ampere_sgemm_128x64_tn                                                                              
     12.4      17765301232       3435    5171849.0      70786.0      32033  103180754   11855625.0  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
     11.4      16225213696       1095   14817546.8    5454695.0     504013  238319195   29832211.6  ampere_sgemm_128x64_nt                                                                              
      7.7      11067873682        180   61488187.1   48763276.5   35828878  108614798   21610699.8  ampere_sgemm_64x64_nt                                                                               
      7.3      10445427151       2940    3552866.4     827301.5        928   35362674    6110356.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…
      5.3       7588221601      10105     750937.3      59074.0        960   48571604    2632606.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<float>, std::arr…
      4.3       6186928885       4385    1410930.2      70018.0      30529   25391524    2979673.5  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      2.2       3155417206        720    4382523.9     555982.0     546990   21070136    6029578.2  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      2.1       3037896815         15  202526454.3  199696044.0  196108113  218822571    6278869.6  void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_nn_align1>(T1::Params)                      
      2.1       3037294733        360    8436929.8     458859.5     403114   42119982   11147744.5  ampere_sgemm_128x128_tn                                                                             
      2.1       2956901378        180   16427229.9     476524.0      68642   52370941   23253249.3  void at::native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<at::native::<unnamed>::OpaqueType<…
      1.9       2771376558        360    7698268.2    1380388.0     606863   48545096   10958430.9  ampere_sgemm_128x128_nn                                                                             
      1.6       2353330924        380    6192976.1     556957.5     552495   31994479    8729124.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::neg_kernel_cuda(at::TensorIterat…
      1.5       2112654137      10740     196709.0      27328.0        928   13088952     556145.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::AUnaryFunctor<float, float, floa…
      1.2       1714183065        930    1843207.6     277639.0      36385   17143812    3432193.9  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…
      1.2       1677122427        360    4658673.4     556878.0     550286   20490387    7222787.2  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, floa…
      1.1       1633473254        370    4414792.6     546574.0     544686   26662669    6415944.2  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…
      1.0       1376455795        360    3823488.3     597199.0     571182   15406454    5023455.8  ampere_sgemm_128x128_nt  

下面是对五小题的简短回答:

(a) 前向传递总耗时约 43,648,762,123 ns ≈ 43.65 s（来自 :forward_pass 的 43648762123 ns），与用 Python 标准库测得的前向时间在量级上是一致的（在测量误差／运行噪声范围内匹配）。

(b) 前向传递中累计 GPU 时间最多的 CUDA kernel 是 ampere_sgemm_128x64_tn（占 38.4%，总时长 ~17.05 s），在单次前向中被调用 1275 次；在完整训练（前向+反向+优化）统计中它不是唯一的第一名（被 ampere_sgemm_128x64_nn 等其它 matmul 内核分摊，变为并列的重要内核）。

(c) 除了矩阵乘（sgemm）外，前向中还有显著占比的内核包括各种 elementwise / vectorized elementwise（点乘、加、exp、sigmoid 等） 与 reduce（如 max/mean 用于 softmax） 内核，这些占用了非小的 CUDA 时间。

(d) 在做完整训练步（forward + backward + optimizer）时，矩阵乘仍然是主要耗时项但其相对比例被反向传递中大量的 elementwise / 向量化内核与梯度相关操作稀释——也就是说相对于只做推理时，矩阵乘占总步骤的比例下降（而 elementwise、归约、拷贝/拼接等内核在整体训练步骤中所占比重上升）。

(e) 在自注意力层内，softmax（由 exp、reduce、除法等 elementwise/归约内核实现）的实际运行时间远小于矩阵乘，但并不会按 FLOPs 比例线性缩小：softmax 的 FLOPs 比矩阵乘少得多，但它更受内存/带宽与零散内核开销影响，因此其 运行时间相对于 FLOPs 的“昂贵性”要更高一些（即软/内存绑定开销使得 softmax 在时间上显得比按 FLOPs 预期要贵）。

Problem (mixed_precision_accumulation): 1 point
Problem:
    Run the following code and commment on the (accuracy of the) results.
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
    print(s)
    s = torch.tensor(0,dtype=torch.float16)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
    print(s)
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
    print(s)
    s = torch.tensor(0,dtype=torch.float32)
    for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
    print(s)
    Deliverable: A 2-3 sentence response

ans:
    tensor(10.0001)
    tensor(9.9531, dtype=torch.float16)
    tensor(10.0021)
    tensor(10.0021)


Problem (benchmarking_mixed_precision):
Problem:
    (a) Consider the following model:
    1 class ToyModel(nn.Module):
    2 def __init__(self, in_features: int, out_features: int):
    3 super().__init__()
    4 self.fc1 = nn.Linear(in_features, 10, bias=False)
    5 self.ln = nn.LayerNorm(10)
    6 self.fc2 = nn.Linear(10, out_features, bias=False)
    7 self.relu = nn.ReLU()
    8
    9 def forward(self, x):
    10 x = self.relu(self.fc1(x))
    11 x = self.ln(x)
    12 x = self.fc2(x)
    13 return x
    Suppose we are training the model on a GPU and that the model parameters are originally in
    FP32. We’d like to use autocasting mixed precision with FP16. What are the data types of:
    • the model parameters within the autocast context,
    • the output of the first feed-forward layer (ToyModel.fc1),
    • the output of layer norm (ToyModel.ln),
    • the model’s predicted logits,
    • the loss,
    • and the model’s gradients?
    Deliverable: The data types for each of the components listed above.
    (b) You should have seen that FP16 mixed precision autocasting treats the layer normalization layer
    differently than the feed-forward layers. What parts of layer normalization are sensitive to mixed
    precision? If we use BF16 instead of FP16, do we still need to treat layer normalization differently?
    Why or why not?
    Deliverable: A 2-3 sentence response.
    (c) Modify your benchmarking script to optionally run the model using mixed precision with BF16.
    Time the forward and backward passes with and without mixed-precision for each language model
    size described in §1.1.2. Compare the results of using full vs. mixed precision, and comment on
    any trends as model size changes. You may find the nullcontext no-op context manager to be
    useful.
    Deliverable: A 2-3 sentence response with your timings and commentary

ans:
    (a)
    | 组件                                  | 数据类型（dtype）     | 原因解释                                                 |
    | ----------------------------------- | --------------- | ---------------------------------------------------- |
    | **模型参数**                            | `torch.float32` | AMP 不会改变参数存储精度。参数仍为 FP32，以避免累积误差。                    |
    | **fc1 输出 (`ToyModel.fc1`)**         | `torch.float16` | Linear 是 matmul 类算子，AMP 会将输入和计算自动转为 FP16。            |
    | **LayerNorm 输出 (`ToyModel.ln`)**    | `torch.float32` | LayerNorm 在 AMP 下强制保持 FP32 精度，因为归一化与除法操作对精度敏感。       |
    | **模型预测 logits (`ToyModel.fc2` 输出)** | `torch.float16` | fc2 是线性层，在 AMP 下以 FP16 执行（输入 FP32 会被临时 cast 为 FP16）。 |
    | **Loss**                            | `torch.float32` | 损失函数通常会在 FP32 下计算（如 `CrossEntropyLoss` 会输出 FP32）。    |
    | **Gradients**                       | `torch.float32` | 梯度在 FP32 master 参数上累积与更新，保持稳定性。                      |

    (b)
    使用BF16时的处理：使用BF16时通常不需要特殊对待LayerNorm。因为BF16具有与FP32相同的指数范围(∼10⁻³⁸ to 10³⁸)，
    虽然精度较低但数值稳定性好，能够安全处理平方和除法操作而不会溢出。

    (c)
    在修改基准测试脚本以支持BF16混合精度时，主要步骤和预期结果如下：
    实现方法：使用torch.autocast上下文管理器，对前向传播和损失计算启用BF16，同时保持优化器在FP32中更新参数。
    预期计时趋势：
    小模型：混合精度可能带来轻微加速或无明显优势
    中大模型：混合精度应显示明显加速（1.3-1.8倍），因为BF16减少了内存带宽需求和计算开销
    超大模型：加速效果最显著，因为内存节省避免了GPU内存交换

    batch_size == 4
    ==== Benchmark Summary (ms per step) ====
    Forward:   708.74 ± 70.38
    Backward:  1122.31 ± 231.31
    Optimizer: 150.70 ± 19.64
    Total:     1986.32 ± 189.67
    =========================================

    batch_size == 2
    ==== Benchmark Summary (ms per step) ====
    Forward:   61.60 ± 0.32
    Backward:  129.18 ± 0.52
    Optimizer: 104.80 ± 0.76
    Total:     304.43 ± 23.73
    =========================================

Problem:
    Problem (pytorch_attention): 2 points
    (a) Benchmark your attention implementation at different scales. Write a script that will:
    (a) Fix the batch size to 8 and don’t use multihead attention (i.e. remove the head dimension).
    (b) Iterate through the cartesian product of [16, 32, 64, 128] for the head embedding dimension dmodel, and [256, 1024, 4096, 8192, 16384] for the sequence length.
    (c) Create random inputs Q, K, V for the appropriate size.
    (d) Time 100 forward passes through attention using the inputs.
    (e) Measure how much memory is in use before the backward pass starts, and time 100 backward
    passes.
    (f) Make sure to warm up, and to call torch.cuda.synchronize() after each forward/backward
    pass.
    Report the timings (or out-of-memory errors) you get for these configurations. At what size do
    you get out-of-memory errors? Do the accounting for the memory usage of attention in one of the
    smallest configurations you find that runs out of memory (you can use the equations for memory
    usage of Transformers from Assignment 1). How does the memory saved for backward change
    with the sequence length? What would you do to eliminate this memory cost?
    Deliverable: A table with your timings, your working out for the memory usage, and a 1-2
    paragraph response.

Solution:
       d_model  seq_len  forward_time_ms  backward_time_ms  memory_before_backward_MB   status
    0       16      256         0.286360          0.636823                  19.125488  Success
    1       16     1024         1.712267          3.668833                  51.750488  Success
    2       32      256         0.269003          0.634968                  20.000488  Success
    3       32     1024         1.730001          3.741384                  55.250488  Success
    4       64      256         0.296972          0.674500                  21.750488  Success
    5       64     1024         1.775091          4.014287                  62.250488  Success

Problem:
    Problem (torch_compile): 2 points
    (a) Extend your attention benchmarking script to include a compiled version of your PyTorch implementation of attention, and compare its performance to the uncompiled version with the same
    configuration as the pytorch_attention problem above.
    Deliverable: A table comparing your forward and backward pass timings for your compiled
    attention module with the uncompiled version from the pytorch_attention problem above.
    (b) Now, compile your entire Transformer model in your end-to-end benchmarking script. How does
    the performance of the forward pass change? What about the combined forward and backward
    passes and optimizer steps?
    Deliverable: A table comparing your vanilla and compiled Transformer model.




Solution:
    (a)
           version  d_model  seq_len  forward_time_ms  backward_time_ms   status
    0   uncompiled       16      256         0.386119          1.463807  Success
    1     compiled       16      256         0.407028          0.812531  Success
    2   uncompiled       16     1024         2.222729          4.298615  Success
    3     compiled       16     1024         1.766145          4.475713  Success
    4   uncompiled       32      256         1.328003          1.709282  Success
    5     compiled       32      256         0.406444          0.776589  Success
    6   uncompiled       32     1024         2.013481          4.145408  Success
    7     compiled       32     1024         1.710641          3.203261  Success
    8   uncompiled       64      256         0.343549          0.793374  Success
    9     compiled       64      256         0.382316          0.719428  Success
    10  uncompiled       64     1024         1.908934          4.085565  Success
    11    compiled       64     1024         1.623893          3.269637  Success

    === Comparison Table (compiled vs uncompiled) ===
                    backward_time_ms            forward_time_ms           
    version                 compiled uncompiled        compiled uncompiled
    d_model seq_len                                                       
    16      256             0.812531   1.463807        0.407028   0.386119
            1024            4.475713   4.298615        1.766145   2.222729
    32      256             0.776589   1.709282        0.406444   1.328003
            1024            3.203261   4.145408        1.710641   2.013481
    64      256             0.719428   0.793374        0.382316   0.343549
            1024            3.269637   4.085565        1.623893   1.908934

    (b)
    === Running vanilla model ===
    Running 5 warm-up steps...
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.48s/it]
    Running 10 measurement steps...
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:39<00:00,  9.95s/it]

    ==== Benchmark Summary (ms per step) ====
    Forward:   3014.62 ± 160.60
    Backward:  6569.98 ± 398.94
    Optimizer: 360.32 ± 103.30
    Total:     9952.94 ± 436.59
    =========================================

    === Running compiled model ===
    Running 5 warm-up steps...
    0%|                                                                                                                              | 0/5 [00:00<?, ?it/s]/home/booft/pytorch-selflearning/assignment2-systems/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
    warnings.warn(
    W1017 16:19:05.785000 19210 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode
    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:06<00:00, 13.24s/it]
    Running 10 measurement steps...
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:10<00:00,  7.00s/it]

    ==== Benchmark Summary (ms per step) ====
    Forward:   944.12 ± 165.17
    Backward:  5701.60 ± 561.36
    Optimizer: 356.77 ± 63.94
    Total:     7003.06 ± 621.03
    =========================================

    ==== Comparison Table (ms per step) ====
            Forward (ms)  Backward (ms)  Optimizer (ms)   Total (ms)
    vanilla    3014.621091    6569.982600      360.318279  9952.937675
    compiled    944.122863    5701.600552      356.773639  7003.055859
    =========================================
    
Question:
    Problem (distributed_communication_single_node): 5 points
    Write a script to benchmark the runtime of the all-reduce operation in the single-node multi-process
    setup. The example code above may provide a reasonable starting point. Experiment with varying the
    following settings:
    Backend + device type: Gloo + CPU, NCCL + GPU.
    all-reduce data size: float32 data tensors ranging over 1MB, 10MB, 100MB, 1GB.
    Number of processes: 2, 4, or 6 processes.
    Resource requirements: Up to 6 GPUs. Each benchmarking run should take less than 5 minutes.

    Deliverable: Plot(s) and/or table(s) comparing the various settings, with 2-3 sentences of commentary about your results and thoughts about how the various factors interact.


Solution : 
    [Backend=gloo, Device=cpu, Size=1MB, World=2]
    Avg all-reduce time (per rank): [0.0025213719345629215, 0.002513241721317172]
    Mean time: 0.0025 s

    [Backend=gloo, Device=cpu, Size=10MB, World=2]
    Avg all-reduce time (per rank): [0.006701040081679821, 0.0067071677185595036]
    Mean time: 0.0067 s

    [Backend=gloo, Device=cpu, Size=100MB, World=2]
    Avg all-reduce time (per rank): [0.06403537094593048, 0.06402194499969482]
    Mean time: 0.0640 s

    [Backend=gloo, Device=cpu, Size=1MB, World=4]
    Avg all-reduce time (per rank): [0.004708766937255859, 0.0047458172775805, 0.004713201429694891, 0.004721975419670343]
    Mean time: 0.0047 s

    [Backend=gloo, Device=cpu, Size=10MB, World=4]
    Avg all-reduce time (per rank): [0.012697577476501465, 0.012697339057922363, 0.012678814120590687, 0.012694573029875755]
    Mean time: 0.0127 s

    [Backend=gloo, Device=cpu, Size=100MB, World=4]
    Avg all-reduce time (per rank): [0.13856235146522522, 0.1385645866394043, 0.13855834305286407, 0.13856299221515656]
    Mean time: 0.1386 s

result on server(cpu):
    [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
    [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1

    [Backend=gloo, Device=cpu, Size=1MB, World=2]
    Avg all-reduce time (per rank): [0.0008834839100018144, 0.0008838653448037803]
    Mean time: 0.0009 s
    [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
    [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1

    [Backend=gloo, Device=cpu, Size=10MB, World=2]
    Avg all-reduce time (per rank): [0.008769440464675426, 0.008771633729338646]
    Mean time: 0.0088 s
    [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
    [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1

    [Backend=gloo, Device=cpu, Size=100MB, World=2]
    Avg all-reduce time (per rank): [0.09392721951007843, 0.09392790496349335]
    Mean time: 0.0939 s

    [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

    [Backend=gloo, Device=cpu, Size=1MB, World=4]
    Avg all-reduce time (per rank): [0.0018239021301269531, 0.0018257379997521639, 0.0018275261390954256, 0.0018243551021441817]
    Mean time: 0.0018 s
    [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

    [Backend=gloo, Device=cpu, Size=10MB, World=4]
    Avg all-reduce time (per rank): [0.013946080580353737, 0.013952994719147682, 0.013945961371064186, 0.013951706700026989]
    Mean time: 0.0139 s
    [Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
    [Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

    [Backend=gloo, Device=cpu, Size=100MB, World=4]
    Avg all-reduce time (per rank): [0.1665104627609253, 0.16651995480060577, 0.1665157526731491, 0.16652266681194305]
    Mean time: 0.1665 s

result on server(cuda):
    [Backend=nccl, Device=cuda, Size=1MB, World=2]
    Avg all-reduce time (per rank): [8.087158494163305e-05, 7.936954352771863e-05]
    Mean time: 0.0001 s

    [Backend=nccl, Device=cuda, Size=10MB, World=2]
    Avg all-reduce time (per rank): [0.0004024505615234375, 0.00040061475010588765]
    Mean time: 0.0004 s

    [Backend=nccl, Device=cuda, Size=100MB, World=2]
    Avg all-reduce time (per rank): [0.0037048577796667814, 0.003703904105350375]
    Mean time: 0.0037 s

    [Backend=nccl, Device=cuda, Size=1MB, World=4]
    Avg all-reduce time (per rank): [0.000186920166015625, 0.0002096176176564768, 0.00020935534848831594, 0.00021376609220169485]
    Mean time: 0.0002 s

    [Backend=nccl, Device=cuda, Size=10MB, World=4]
    Avg all-reduce time (per rank): [0.0007302522426471114, 0.000740861869417131, 0.000737738620955497, 0.0007498025661334395]
    Mean time: 0.0007 s


    [Backend=nccl, Device=cuda, Size=100MB, World=4]
    Avg all-reduce time (per rank): [0.006644559092819691, 0.006653857417404652, 0.006651639938354492, 0.0066516161896288395]
    Mean time: 0.0067 s

Analysis:

    | Backend | Device     | World Size | Data Size | 平均时间 (s) | 备注            |
    | ------- | ---------- | ---------- | --------- | -------- | ------------- |
    | gloo    | CPU        | 2          | 1MB       | 0.0009   | 慢于 GPU 两个数量级  |
    | gloo    | CPU        | 2          | 100MB     | 0.094    | 线性增长明显        |
    | gloo    | CPU        | 4          | 100MB     | 0.166    | 增加进程后通信开销倍增   |
    | nccl    | GPU (5090) | 2          | 1MB       | 0.00008  | 极快，~10μs 级    |
    | nccl    | GPU (5090) | 2          | 100MB     | 0.0037   | 仅 3.7ms，优异    |
    | nccl    | GPU (5090) | 4          | 100MB     | 0.0067   | 稍有上升，通信同步代价增加 |

    GPU（NCCL）比 CPU（Gloo）快约 25～40倍；

    GPU 的 时间随数据规模几乎线性增长，说明带宽占主导；

    增加 world_size（进程数）后，时间略增但不爆炸，说明通信算法优化得好；

    CPU 的增长曲线更陡，说明其受限于网络栈和内存带宽。


Problem:
    Problem (naive_ddp_benchmarking): 3 points
    In this naïve DDP implementation, parameters are individually all-reduced across ranks after each
    backward pass. To better understand the overhead of data parallel training, create a script to benchmark your previously-implemented language model when trained with this naïve implementation of
    DDP. Measure the total time per training step and the proportion of time spent on communicating
    gradients. Collect measurements in the single-node setting (1 node x 2 GPUs) for the XL model size
    described in §1.1.2.
    Deliverable: A description of your benchmarking setup, along with the measured time per training
    iteration and time spent communicating gradients for each setting

Solution(my measurement on 2 * 5090 GPU, twice):
    ============================================================
    Naive DDP Benchmark - XL Model
    ============================================================
    Model parameters: 1.56B
    World size: 2
    Per-GPU batch size: 4
    Sequence length: 32
    Training steps: 10 (warmup: 5)
    ============================================================

    Step   0 | Total:  940.2ms | Forward:  252.9ms | Backward:   85.9ms | Comm:  309.9ms ( 33.0%)
    Step   1 | Total:  401.7ms | Forward:   22.6ms | Backward:   32.5ms | Comm:  260.9ms ( 64.9%)
    Step   2 | Total:  412.9ms | Forward:   22.7ms | Backward:   43.2ms | Comm:  261.5ms ( 63.3%)
    Step   3 | Total:  411.8ms | Forward:   22.4ms | Backward:   43.0ms | Comm:  260.8ms ( 63.3%)
    Step   4 | Total:  415.3ms | Forward:   22.5ms | Backward:   46.7ms | Comm:  260.7ms ( 62.8%)
    Step   5 | Total:  421.1ms | Forward:   22.3ms | Backward:   53.2ms | Comm:  260.2ms ( 61.8%)

    ============================================================
    Benchmark Results (averaged over 5 steps)
    ============================================================
    Average time per training step:       422.09 ms
    - Forward pass:                       22.54 ms
    - Backward pass:                      53.33 ms
    - Gradient communication:            260.72 ms

    Communication overhead:                  61.8 %
    ============================================================

    Key Observations:
    1. Naive DDP communicates 1557.6M parameters
    2. Each parameter is all-reduced individually (high overhead)
    3. Communication takes 61.8% of total training time
    4. This overhead can be reduced by bucketing gradients


    ============================================================
    Naive DDP Benchmark - XL Model
    ============================================================
    Model parameters: 1.56B
    World size: 2
    Per-GPU batch size: 4
    Sequence length: 32
    Training steps: 10 (warmup: 5)
    ============================================================

    Step   0 | Total:  966.3ms | Forward:  260.3ms | Backward:  115.9ms | Comm:  263.7ms ( 27.3%)
    Step   1 | Total:  398.2ms | Forward:   22.6ms | Backward:   29.2ms | Comm:  260.7ms ( 65.5%)
    Step   2 | Total:  411.6ms | Forward:   22.6ms | Backward:   42.8ms | Comm:  260.5ms ( 63.3%)
    Step   3 | Total:  412.9ms | Forward:   22.5ms | Backward:   44.2ms | Comm:  260.7ms ( 63.1%)
    Step   4 | Total:  419.0ms | Forward:   22.6ms | Backward:   50.3ms | Comm:  260.7ms ( 62.2%)
    Step   5 | Total:  418.0ms | Forward:   22.6ms | Backward:   49.6ms | Comm:  260.4ms ( 62.3%)

    ============================================================
    Benchmark Results (averaged over 5 steps)
    ============================================================
    Average time per training step:       420.51 ms
    - Forward pass:                       22.60 ms
    - Backward pass:                      50.68 ms
    - Gradient communication:            261.62 ms

    Communication overhead:                  62.2 %
    ============================================================

    Key Observations:
    1. Naive DDP communicates 1557.6M parameters
    2. Each parameter is all-reduced individually (high overhead)
    3. Communication takes 62.2% of total training time
    4. This overhead can be reduced by bucketing gradients